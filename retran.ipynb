{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76b70ad0",
   "metadata": {},
   "source": [
    "# Attention Ain't All, Recurent Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842f5287",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7564a825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, math, torch, time, torchtune, datasets, sys, os, json\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from pysat.solvers import Solver\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a92be0",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac0a44c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = 512 * 1000\n",
    "valid_samples = train_samples // 5\n",
    "test_samples = train_samples // 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fe44d82-3eff-4f8c-97da-e6aaca8f88ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "START = '<start>'\n",
    "STOP = '<stop>'\n",
    "PAD = '<pad>'\n",
    "SEP = '='\n",
    "\n",
    "# Translates from a string to a pytorch tensor using a vocab\n",
    "def encode(string, vocab, pad_length):\n",
    "    out = []\n",
    "    while len(string) > 0:\n",
    "        vocab_match = False\n",
    "        for i in range(1, len(string) + 1):\n",
    "            if string[:i] in vocab:\n",
    "                out.append(vocab.index(string[:i]))\n",
    "                string = string[i:]\n",
    "                vocab_match = True\n",
    "                break\n",
    "        if not vocab_match:\n",
    "            raise Exception(\"Encoding error:\", string, vocab)\n",
    "    out += [vocab.index(PAD)] * (pad_length - len(out))\n",
    "    return torch.tensor(out, dtype=torch.long)\n",
    "\n",
    "# Translates from a pytorch tensor to a string using a vocab\n",
    "def decode(tensor, vocab):\n",
    "    ans = \"\"\n",
    "    for i in tensor:\n",
    "        if vocab[i] == START or vocab[i] == PAD:\n",
    "            continue\n",
    "        if vocab[i] == STOP:\n",
    "            break\n",
    "        ans += vocab[i]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "957b2d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arithmetic dataset generated using probablistic context free grammar\n",
    "# Consists of integers, +, -, *, //, and %\n",
    "class ArithDataset(torch.utils.data.Dataset):\n",
    "    vocab = [START, SEP, STOP, PAD] + [str(i) for i in range(10)] + ['-', '+', '*',  '%', '//', '(', ')']\n",
    "\n",
    "    # Probablistic context free grammar rules\n",
    "    # Dictionary where key is the token to be expanded and\n",
    "    # right side is list of tuples containing rules and associated probabilities\n",
    "    rules = {\n",
    "        'EQ': [(['VAL', 'OP', 'VAL'], 0.5), (['(', 'VAL', 'OP', 'VAL', ')'], 0.5)],\n",
    "        'VAL': [(['EQ'], 0.45), (['NUM'], 0.55)],\n",
    "        'OP': [(['-'], 0.2), (['+'], 0.2), (['*'], 0.2), (['%'], 0.2), (['//'], 0.2)],\n",
    "        'NUM': [([str(i), 'NUMT'], 1.0/19) for i in range(1, 10)] + [(['-', str(i), 'NUMT'], 1.0/19) for i in range(1, 10)] + [(['0'], 1.0/19)],\n",
    "        'NUMT': [([str(i), 'NUMT'], 0.2/10) for i in range(10)] + [([], 0.8)],\n",
    "    }\n",
    "\n",
    "    # Chooses a rule based on the probabilities\n",
    "    def selectRule(left_hand):\n",
    "        selector = random.random()\n",
    "        for i in ArithDataset.rules[left_hand]:\n",
    "            selector -= i[1]\n",
    "            if(selector < 0):\n",
    "                return i[0]\n",
    "        raise Exception(\"Improper rule probabilities\")\n",
    "\n",
    "    # Initialize dataset with certain bounds\n",
    "    # Bounds relate to the number of tokens, not characters\n",
    "    # X relates to the arithmetic problem, Y the solution\n",
    "    def __init__(self, samples, seed=0, min_x_len=5, max_x_len=30, min_y_len=1, max_y_len=8):\n",
    "        self.min_x_len = min_x_len\n",
    "        self.max_x_len = max_x_len\n",
    "        self.min_y_len = min_y_len\n",
    "        self.max_y_len = max_y_len\n",
    "        \n",
    "        dup_check = set()\n",
    "        self.xy = []\n",
    "        \n",
    "        random.seed(seed)\n",
    "        while (len(dup_check) < samples):\n",
    "            x, y = self.generateProblem()\n",
    "            if x not in dup_check:\n",
    "                if len(dup_check) % 100 == 0:\n",
    "                    sys.stdout.write(f\"\\r {len(dup_check) / samples * 100:.2f}% complete\")\n",
    "                    sys.stdout.flush()\n",
    "                self.xy.append(encode(START + x + SEP + y + STOP, ArithDataset.vocab, max_x_len + max_y_len))\n",
    "                dup_check.add(x)\n",
    "        print(\"\\r100.00% complete\")\n",
    "        \n",
    "        self.xy = torch.stack(self.xy)\n",
    "        self.x_lens = (self.xy == ArithDataset.vocab.index(SEP)).nonzero()[:, 1] + 1\n",
    "        self.xy_lens = (self.xy == ArithDataset.vocab.index(STOP)).nonzero()[:, 1] + 1\n",
    "\n",
    "    # Generate a problem using the context free grammar within dataset bounds\n",
    "    def generateProblem(self):\n",
    "        while(True):\n",
    "            stack = ['EQ']\n",
    "            index = 0\n",
    "            while (index < len(stack) and len(stack) <= (self.max_x_len - 2)): # Subtract 2 for start and sep token\n",
    "                if stack[index] in ArithDataset.rules:\n",
    "                    stack = stack[:index] + ArithDataset.selectRule(stack[index]) + stack[index + 1:]\n",
    "                else:\n",
    "                    index += 1\n",
    "\n",
    "            if len(stack) > (self.max_x_len - 2) or len(stack) < (self.min_x_len - 2):\n",
    "                continue\n",
    "            \n",
    "            try: # Catch division or modulus by 0\n",
    "                x = ''.join(stack)\n",
    "                y = str(eval(x))\n",
    "                if (len(y) >= (self.min_y_len - 1) and len(y) <= (self.max_y_len - 1)): # Subtract 1 for stop token\n",
    "                    return x, y\n",
    "            \n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.xy)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.xy[idx], self.x_lens[idx], self.xy_lens[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45dd8664-c07f-43de-bbc1-4f6f7be0e30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating arithmetic dataset\n",
      "100.00% complete\n",
      "Arithmetic Problems:\n",
      "42--5=47\n",
      "0%(0-2)=0\n",
      "((-9//-2)%(-7--9))=0\n",
      "(-2+60*(-3//5+(-1*1)))=-122\n",
      "-2*-6%-4+2=2\n",
      "(12//-3+9)%6--8=13\n",
      "(-26--3)=-23\n",
      "-7%-794=-7\n",
      "(-7%(9*6*9//(5+7%848)*60))=2393\n",
      "(-8+(-2//6*(-8+-3)//-5*5))=-23\n",
      "(1*((-43%-6)*21)--883-2)=860\n",
      "-6-(5//6)=-6\n",
      "(4-57)+7-6=-52\n",
      "-7+(-1%-15)=-8\n",
      "(((-8*0--4)*9)*-6)=-216\n",
      "(5//(-8*-5+(-5%-9%-4)))=0\n",
      "(-5%-56)=-5\n",
      "(3//(-6*-8)//2%5)=0\n",
      "-726+9=-717\n",
      "(8%(-4*-1)-5*(0+0*4))=0\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"arith_dataset.pt\"):\n",
    "    print(\"Loading saved arithmetic dataset\")\n",
    "    # Creates a warning since weights_only is False, make sure you trust this file and it hasn't been tampered\n",
    "    arithSets = torch.load(\"arith_dataset.pt\")\n",
    "else:\n",
    "    print(\"Generating arithmetic dataset\")\n",
    "    arithData = ArithDataset(train_samples + valid_samples + test_samples)\n",
    "    arithSets = torch.utils.data.random_split(arithData, [train_samples, valid_samples, test_samples], generator=torch.Generator().manual_seed(0))\n",
    "    torch.save(arithSets, 'arith_dataset.pt')\n",
    "    \n",
    "print(\"Arithmetic Problems:\")\n",
    "for i in range(20):\n",
    "    print(decode(arithSets[0][i][0], ArithDataset.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8ed0735-4dd0-4aa0-ae2d-219a651c7cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integer to hexadecimal dataset\n",
    "class HexDataset(torch.utils.data.Dataset):\n",
    "    vocab = [START, SEP, STOP, PAD] + [str(i) for i in range(10)] + ['a', 'b', 'c', 'd', 'e', 'f']\n",
    "\n",
    "    # Initialize dataset with certain bounds\n",
    "    # We generate numbers on an exponential scale to get variety of lengths and difficulties\n",
    "    def __init__(self, samples, seed=0, min_exp=0, max_exp=10):\n",
    "        self.min_exp = min_exp\n",
    "        self.max_exp = max_exp\n",
    "        \n",
    "        dup_check = set()\n",
    "        self.xy = []\n",
    "\n",
    "        max_len = len(str(int(10 ** max_exp))) + len(hex(int(10 ** max_exp))[2:]) + 3 # Crop out 0x, add 3 for start, sep, stop\n",
    "        \n",
    "        random.seed(seed)\n",
    "        while (len(dup_check) < samples):\n",
    "            x = int(10 ** ((max_exp - min_exp) * random.random() + min_exp))\n",
    "            if x not in dup_check:\n",
    "                if len(dup_check) % 100 == 0:\n",
    "                    sys.stdout.write(f\"\\r {len(dup_check) / samples * 100:.2f}% complete\")\n",
    "                    sys.stdout.flush()\n",
    "                self.xy.append(encode(START + str(x) + SEP + hex(x)[2:] + STOP, HexDataset.vocab, max_len))\n",
    "                dup_check.add(x)\n",
    "        print(\"\\r100.00% complete\")\n",
    "        \n",
    "        self.xy = torch.stack(self.xy)\n",
    "        self.x_lens = (self.xy == ArithDataset.vocab.index(SEP)).nonzero()[:, 1] + 1\n",
    "        self.xy_lens = (self.xy == ArithDataset.vocab.index(STOP)).nonzero()[:, 1] + 1\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.xy)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.xy[idx], self.x_lens[idx], self.xy_lens[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c00886f6-00a6-4e99-b357-7591a58204a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating hex dataset\n",
      "100.00% complete\n",
      "Hex Problems:\n",
      "221189=36005\n",
      "24270123=172552b\n",
      "865402=d347a\n",
      "1778378=1b22ca\n",
      "5428030040=143892a58\n",
      "10710=29d6\n",
      "1456108752=56ca74d0\n",
      "7502585671=1bf305f47\n",
      "21217263=143bfef\n",
      "4237792084=fc979354\n",
      "8176414183=1e75a2de7\n",
      "39793=9b71\n",
      "162829345=9b49421\n",
      "4967674291=12818b1b3\n",
      "4306896207=100b6054f\n",
      "59031271=384bee7\n",
      "464980=71854\n",
      "1116773171=42909b33\n",
      "48527=bd8f\n",
      "1977861757=75e3c67d\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"hex_dataset.pt\"):\n",
    "    print(\"Loading saved hex dataset\")\n",
    "    # Creates a warning since weights_only is False, make sure you trust this file and it hasn't been tampered\n",
    "    hexSets = torch.load(\"hex_dataset.pt\")\n",
    "else:\n",
    "    print(\"Generating hex dataset\")\n",
    "    hexData = HexDataset(train_samples + valid_samples + test_samples)\n",
    "    hexSets = torch.utils.data.random_split(hexData, [train_samples, valid_samples, test_samples], generator=torch.Generator().manual_seed(0))\n",
    "    torch.save(hexSets, 'hex_dataset.pt')\n",
    "    \n",
    "print(\"Hex Problems:\")\n",
    "for i in range(20):\n",
    "    print(decode(hexSets[0][i][0], HexDataset.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a50a5e8-e6c7-4e32-b908-c0c860658134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a randomized 3sat problem with specific count of clauses and variables (token version, not propper)\n",
    "# The problems are generally hardest when there is ~4.26 clauses per variable\n",
    "def generate3Sat(num_clauses, num_variables):\n",
    "    # Sets to remove duplicate clauses\n",
    "    clauses = set()\n",
    "    while len(clauses) < num_clauses:\n",
    "        clauses.add(frozenset([i if random.random() > 0.5 else -i for i in random.sample(range(1, num_variables + 1), 3)]))\n",
    "    for var in range(1, num_variables + 1):\n",
    "        if sum([var in clause for clause in clauses]) == 0:\n",
    "            return generate3Sat(num_clauses, num_variables)\n",
    "    return 'and'.join(['or'.join([f'not{-var}' if var < 0 else f\"{var}\" for var in random.sample(list(clause), 3)]) for clause in random.sample(list(clauses), num_clauses)])\n",
    "\n",
    "# This function takes raw token decodings of 3-sat problems and makes them propper\n",
    "def decodeTo3Sat(string):\n",
    "    if '=' in string:\n",
    "        return ' and '.join(['(' + ' or '.join([ 'x' + var for var in clause.split('or')]) + ')' for clause in string[:string.index('=')].split('and')]).replace(\"xnot\", \"not x\") + ' is ' + string[string.index('=') + 1:]\n",
    "    return ' and '.join(['(' + ' or '.join([ 'x' + var for var in clause.split('or')]) + ')' for clause in string.split('and')]).replace(\"xnot\", \"not x\")\n",
    "\n",
    "# Decode the raw tokens to an array that can be parsed by a sat solver\n",
    "def decodeToArray(string):\n",
    "    return eval('[[' + string.replace('not', '-').replace('or', ', ').replace('and', '], [') + ']]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b3f6305-3351-49fe-a916-2fa4e9816fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset of 3-sat problems where the model predicts wether or not the problem is sat\n",
    "class SatSolveDataset(torch.utils.data.Dataset):\n",
    "    vocab = [START, SEP, STOP, PAD] + [str(i) for i in range(10)] + ['and', 'or', 'not', 'sat', 'unsat']\n",
    "\n",
    "    def __init__(self, samples, seed=0, min_vars=4, max_vars=12):\n",
    "        if min_vars < 4:\n",
    "            raise Exception(\"Must have at least 4 variables\")\n",
    "        self.min_vars = min_vars\n",
    "        self.max_vars = max_vars\n",
    "        self.num_sat = 0\n",
    "        self.num_unsat = 0\n",
    "\n",
    "        dup_check = set()\n",
    "        self.xy = []\n",
    "\n",
    "        # max len = max numclauses * ((max variable name len + 1 'not') * variables per clause + 2 'or's and 1 'and' and up to 3 'not's\n",
    "        # + 1 start, 1 sep, 1 sat/unsat, and 1 stop, - 1 extra and\n",
    "        # Now this max length is likely never reached so we will trim at the end of initialization\n",
    "        max_len = max_vars * 5 *  (len(str(max_vars)) * 3 + 6) + 3\n",
    "        \n",
    "        while (len(dup_check) < samples):\n",
    "            # The hardest satisfiability problems occur when there are ~ 4.26 clauses per variable\n",
    "            # to create consistently hard problems with some variation, we generate problems where there are\n",
    "            # between 4 and 5 clauses per variable\n",
    "            num_variables = random.randint(min_vars, max_vars)\n",
    "            num_clauses = random.randint(num_variables * 4, num_variables * 5)\n",
    "            x = generate3Sat(num_clauses, num_variables)\n",
    "            if x not in dup_check:\n",
    "                if len(dup_check) % 100 == 0:\n",
    "                    sys.stdout.write(f\"\\r {len(dup_check) / samples * 100:.2f}% complete\")\n",
    "                    sys.stdout.flush()\n",
    "                cnf_array = decodeToArray(x)\n",
    "                with Solver(name='g3') as solver:\n",
    "                    for clause in cnf_array:\n",
    "                        solver.add_clause(clause)\n",
    "                    satisfiable = solver.solve()\n",
    "                if satisfiable:\n",
    "                    self.xy.append(encode(START + x + SEP + 'sat' + STOP, SatSolveDataset.vocab, max_len))\n",
    "                else:\n",
    "                    self.xy.append(encode(START + x + SEP + 'unsat' + STOP, SatSolveDataset.vocab, max_len))\n",
    "                dup_check.add(x)\n",
    "        print(\"\\r100.00% complete\")\n",
    "\n",
    "        self.xy = torch.stack(self.xy)\n",
    "        self.x_lens = (self.xy == ArithDataset.vocab.index(SEP)).nonzero()[:, 1] + 1\n",
    "        self.xy_lens = (self.xy == ArithDataset.vocab.index(STOP)).nonzero()[:, 1] + 1\n",
    "\n",
    "        # Trim the max length\n",
    "        self.xy = self.xy[:, :torch.max(self.xy_lens)]\n",
    "                    \n",
    "    def __len__(self):\n",
    "        return len(self.xy)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.xy[idx], self.x_lens[idx], self.xy_lens[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c2e00cc-cc02-40cd-b309-c06cf2b7708c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sat solve dataset\n",
      " 93.39% complete"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGenerating sat solve dataset\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     satSolveData = \u001b[43mSatSolveDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_samples\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_samples\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     satSolveSets = torch.utils.data.random_split(satSolveData, [train_samples, valid_samples, test_samples], generator=torch.Generator().manual_seed(\u001b[32m0\u001b[39m))\n\u001b[32m      9\u001b[39m     torch.save(satSolveSets, \u001b[33m'\u001b[39m\u001b[33msat_solve_dataset.pt\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mSatSolveDataset.__init__\u001b[39m\u001b[34m(self, samples, seed, min_vars, max_vars)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Solver(name=\u001b[33m'\u001b[39m\u001b[33mg3\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m solver:\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m clause \u001b[38;5;129;01min\u001b[39;00m cnf_array:\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m         \u001b[43msolver\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_clause\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclause\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m     satisfiable = solver.solve()\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m satisfiable:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/trident/retran/venv/lib/python3.12/site-packages/pysat/solvers.py:1355\u001b[39m, in \u001b[36mSolver.add_clause\u001b[39m\u001b[34m(self, clause, no_return)\u001b[39m\n\u001b[32m   1327\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1328\u001b[39m \u001b[33;03m    This method is used to add a single clause to the solver. An\u001b[39;00m\n\u001b[32m   1329\u001b[39m \u001b[33;03m    optional argument ``no_return`` controls whether or not to check\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1351\u001b[39m \u001b[33;03m        False\u001b[39;00m\n\u001b[32m   1352\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1354\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.solver:\n\u001b[32m-> \u001b[39m\u001b[32m1355\u001b[39m     res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msolver\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_clause\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclause\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_return\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1356\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_return:\n\u001b[32m   1357\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/trident/retran/venv/lib/python3.12/site-packages/pysat/solvers.py:3787\u001b[39m, in \u001b[36mGlucose3.add_clause\u001b[39m\u001b[34m(self, clause, no_return)\u001b[39m\n\u001b[32m   3782\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3783\u001b[39m \u001b[33;03m    Add a new clause to solver's internal formula.\u001b[39;00m\n\u001b[32m   3784\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3786\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.glucose:\n\u001b[32m-> \u001b[39m\u001b[32m3787\u001b[39m     res = \u001b[43mpysolvers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mglucose3_add_cl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mglucose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclause\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m res == \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m   3790\u001b[39m         \u001b[38;5;28mself\u001b[39m.status = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"sat_solve_dataset.pt\"):\n",
    "    print(\"Loading saved sat solve dataset\")\n",
    "    # Creates a warning since weights_only is False, make sure you trust this file and it hasn't been tampered\n",
    "    satSolveSets = torch.load(\"sat_solve_dataset.pt\")\n",
    "else:\n",
    "    print(\"Generating sat solve dataset\")\n",
    "    satSolveData = SatSolveDataset(train_samples + valid_samples + test_samples)\n",
    "    satSolveSets = torch.utils.data.random_split(satSolveData, [train_samples, valid_samples, test_samples], generator=torch.Generator().manual_seed(0))\n",
    "    torch.save(satSolveSets, 'sat_solve_dataset.pt')\n",
    "\n",
    "print(\"Satisfiability Solver Problems:\")\n",
    "for i in range(5):\n",
    "    print(decodeTo3Sat(decode(satSolveSets[0][i][0], SatSolveDataset.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ac8e7d-1549-48e5-b790-dbb86cc5ea59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset of 3-sat problems where the model predicts the one sat solution\n",
    "class SingleSatDataset(torch.utils.data.Dataset):\n",
    "    vocab = [START, SEP, STOP, PAD] + [str(i) for i in range(10)] + ['and', 'or', 'not', 'True', 'False']\n",
    "\n",
    "    def __init__(self, samples, seed=0, min_vars=4, max_vars=12):\n",
    "        if min_vars < 4:\n",
    "            raise Exception(\"Must have at least 4 variables\")\n",
    "        self.min_vars = min_vars\n",
    "        self.max_vars = max_vars\n",
    "\n",
    "        dup_check = set()\n",
    "        self.xy = []\n",
    "\n",
    "        # max len = max numclauses * ((max variable name len + 1 'not') * variables per clause + 2 'or's and 1 'and' and up to 3 'not's\n",
    "        # + 1 start, 1 sep, 1 sat/unsat, and 1 stop, - 1 extra and\n",
    "        # Now this max length is likely never reached so we will trim at the end of initialization\n",
    "        max_len = max_vars * 5 *  (len(str(max_vars)) * 3 + 6) + 3\n",
    "        \n",
    "        while (len(dup_check) < samples):\n",
    "            # The hardest satisfiability problems occur when there are ~ 4.26 clauses per variable\n",
    "            # to create consistently hard problems with some variation, we generate problems where there are\n",
    "            # between 4 and 5 clauses per variable\n",
    "            num_variables = random.randint(min_vars, max_vars)\n",
    "            num_clauses = random.randint(num_variables * 4, num_variables * 5)\n",
    "            x = generate3Sat(num_clauses, num_variables)\n",
    "            if x not in dup_check:\n",
    "                cnf_array = decodeToArray(x)\n",
    "                with Solver(name='g3') as solver:\n",
    "                    for clause in cnf_array:\n",
    "                        solver.add_clause(clause)\n",
    "                    if not solver.solve():\n",
    "                        continue\n",
    "                    model = solver.get_model()\n",
    "                    blocking_clause = [-lit for lit in model]\n",
    "                    solver.add_clause(blocking_clause)\n",
    "                    if solver.solve():\n",
    "                        continue\n",
    "                    self.xy.append(encode(START + x + SEP + ''.join([str(i>0) for i in model]) + STOP, SingleSatDataset.vocab, max_len))\n",
    "                if len(dup_check) % 100 == 0:\n",
    "                    sys.stdout.write(f\"\\r {len(dup_check) / samples * 100:.2f}% complete\")\n",
    "                    sys.stdout.flush()\n",
    "                dup_check.add(x)\n",
    "        print(\"100.00% complete\")\n",
    "\n",
    "        self.xy = torch.stack(self.xy)\n",
    "        self.x_lens = (self.xy == ArithDataset.vocab.index(SEP)).nonzero()[:, 1] + 1\n",
    "        self.xy_lens = (self.xy == ArithDataset.vocab.index(STOP)).nonzero()[:, 1] + 1\n",
    "\n",
    "        # Trim the max length\n",
    "        self.xy = self.xy[:, :torch.max(self.xy_lens)]\n",
    "                    \n",
    "    def __len__(self):\n",
    "        return len(self.xy)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.xy[idx], self.x_lens[idx], self.xy_lens[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a18429-3360-4d17-bb47-0d3bb7bb97da",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"single_sat_dataset.pt\"):\n",
    "    print(\"Loading saved single sat dataset\")\n",
    "    # Creates a warning since weights_only is False, make sure you trust this file and it hasn't been tampered\n",
    "    singleSatSets = torch.load(\"single_sat_dataset.pt\")\n",
    "else:\n",
    "    print(\"Generating single sat dataset\")\n",
    "    singleSatData = SingleSatDataset(train_samples + valid_samples + test_samples)\n",
    "    singleSatSets = torch.utils.data.random_split(singleSatData, [train_samples, valid_samples, test_samples], generator=torch.Generator().manual_seed(0))\n",
    "    torch.save(singleSatSets, 'single_sat_dataset.pt')\n",
    "\n",
    "print(\"Single Satisfiable Problems:\")\n",
    "for i in range(5):\n",
    "    print(decodeTo3Sat(decode(singleSatSets[0][i][0], SingleSatDataset.vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e93880",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "597cd48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic feed forward, uses GELU activation and GPT-2 initialization\n",
    "class FeedFwd(nn.Module):\n",
    "    def __init__(self, dims, dropout=0.1, activ=nn.GELU()):\n",
    "        super(FeedFwd, self).__init__()\n",
    "        layers = [nn.Linear(dims[i], dims[i+1]) for i in range(len(dims) - 1)]\n",
    "        for i in layers:\n",
    "            torch.nn.init.normal_(i.weight, mean=0.0, std=0.02)\n",
    "            nn.init.zeros_(i.bias)\n",
    "        self.lays = nn.ModuleList(layers)\n",
    "        self.activ = activ\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.lays[:-1]:\n",
    "            x = self.drop(self.activ(layer(x)))\n",
    "        return self.lays[-1](x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5039894-b147-4d26-a507-64ba939b5967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long short-term memory network with projection\n",
    "# Utilizes orthogonal initialization for recurrent weights and GPT-2 initialization for others\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, d_in, d_long, d_short):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.d_in = d_in\n",
    "        self.d_long = d_long\n",
    "        self.d_short = d_short\n",
    "        \n",
    "        self.long0 = nn.Parameter(torch.zeros(d_long))\n",
    "        self.short0 = nn.Parameter(torch.zeros(d_short))\n",
    "        self.i_short = nn.Linear(d_short, d_long, bias=False)\n",
    "        self.i_in = nn.Linear(d_in, d_long)\n",
    "        self.f_short = nn.Linear(d_short, d_long, bias=False)\n",
    "        self.f_in = nn.Linear(d_in, d_long)\n",
    "        self.g_short = nn.Linear(d_short, d_long, bias=False)\n",
    "        self.g_in = nn.Linear(d_in, d_long)\n",
    "        self.o_short = nn.Linear(d_short, d_long, bias=False)\n",
    "        self.o_in = nn.Linear(d_in, d_long)\n",
    "        self.proj = nn.Linear(d_long, d_short)\n",
    "        self.out = nn.Linear(d_long, d_short)\n",
    "\n",
    "        nn.init.ones_(self.f_in.bias)\n",
    "        for bias in [self.i_in.bias, self.g_in.bias, self.o_in.bias, self.proj.bias, self.out.bias]:\n",
    "            nn.init.zeros_(bias)\n",
    "\n",
    "        for weight in [self.i_short.weight, self.f_short.weight, self.g_short.weight, self.o_short.weight, self.proj.weight]:\n",
    "            nn.init.orthogonal_(weight)\n",
    "\n",
    "        for weight in [self.i_in.weight, self.f_in.weight, self.g_in.weight, self.o_in.weight, self.out.weight]:\n",
    "            nn.init.normal_(weight, mean=0.0, std=0.02)\n",
    "            \n",
    "    def forward(self, x, x_len, hiddens = None):\n",
    "        batch_size = x.size(0)\n",
    "        sq_len = x.size(1)\n",
    "        if hiddens == None:\n",
    "            shorts = self.short0.unsqueeze(0).repeat(batch_size, 1)\n",
    "            longs = self.long0.unsqueeze(0).repeat(batch_size, 1)\n",
    "        else:\n",
    "            shorts = hiddens[0]\n",
    "            longs = hiddens[1]\n",
    "            \n",
    "        mask = (torch.arange(sq_len, device=x.device).unsqueeze(0) < x_len.unsqueeze(1)).unsqueeze(-1)\n",
    "        out = torch.zeros(batch_size, sq_len, self.d_short, device=x.device)\n",
    "        for sq_idx in range(sq_len):\n",
    "            i = torch.sigmoid(self.i_short(shorts) + self.i_in(x[:, sq_idx]))\n",
    "            f = torch.sigmoid(self.f_short(shorts) + self.f_in(x[:, sq_idx]))\n",
    "            g = torch.tanh(self.g_short(shorts) + self.g_in(x[:, sq_idx]))\n",
    "            o = torch.sigmoid(self.o_short(shorts) + self.o_in(x[:, sq_idx]))\n",
    "            longs = (f * longs + i * g) * mask[:, sq_idx] + longs * ~mask[:, sq_idx]\n",
    "            shorts = self.proj(o * longs) * mask[:, sq_idx] + shorts * ~mask[:, sq_idx]\n",
    "            out[:, sq_idx] = self.out(o * longs)\n",
    "        return out, (shorts, longs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "731e2563-3f15-4b39-94d2-f577b0849817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-head self attention with RoPE defined in a manner that allows both parallel and auto-regressive computation\n",
    "float_min = torch.finfo(torch.float32).min\n",
    "class MHSA(nn.Module):\n",
    "    def __init__(self, d_model, d_sa, n_head, dropout = 0.1):\n",
    "        super(MHSA, self).__init__()\n",
    "        self.n_head = n_head\n",
    "        self.d_model = d_model\n",
    "        self.d_sa = d_sa\n",
    "        self.d_key = d_sa // n_head\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.rope = torchtune.modules.RotaryPositionalEmbeddings(dim=self.d_key)\n",
    "        \n",
    "        self.q = nn.Linear(d_model, d_sa)\n",
    "        self.k = nn.Linear(d_model, d_sa)\n",
    "        self.v = nn.Linear(d_model, d_sa)\n",
    "        self.sa_lin = nn.Linear(d_sa, d_model)\n",
    "        for lin in [self.k, self.q, self.v, self.sa_lin]:\n",
    "            nn.init.normal_(lin.weight, mean=0.0, std=0.02)\n",
    "            nn.init.zeros_(lin.bias)\n",
    "\n",
    "    def forward(self, x, ks, vs, mask=None, position=None):\n",
    "        # If we are receiving one single element in a sequence rather than a whole sequence,\n",
    "        # we unsqueeze it at the beginning and resqueeze it at the end to make the tensor shapes work out\n",
    "        is_sequence = (len(x.size()) == 3)\n",
    "        if not is_sequence:\n",
    "            x = x.unsqueeze(1)\n",
    "        batch_size = x.size(0)\n",
    "        sq_len = x.size(1)\n",
    "        qs = self.rope(self.q(x).view(batch_size, sq_len, self.n_head, self.d_key), input_pos=position).transpose(1, 2)\n",
    "        dots = torch.matmul(qs, ks.transpose(-1, -2)) / math.sqrt(self.d_key)\n",
    "        if mask != None:\n",
    "            dots[~(mask.unsqueeze(1).repeat(1, self.n_head, 1, 1))] = float_min\n",
    "        attn_weight = self.dropout(nn.Softmax(dim = -1)(dots))\n",
    "        attns = torch.matmul(attn_weight, vs).transpose(1, 2).contiguous().view(batch_size, sq_len, self.d_sa)\n",
    "        if not is_sequence:\n",
    "            attns = attns.squeeze(1)\n",
    "        return self.sa_lin(attns)\n",
    "\n",
    "    # Cacheing keys and values is extremely important for autoregressive generation\n",
    "    # This saves significant computation for recurrent architectures\n",
    "    def cache_kvs(self, x, ks=None, vs=None, position=None):\n",
    "        if len(x.size()) == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        if ks == None:\n",
    "            sq_len = x.size(1)\n",
    "            ks = self.rope(self.k(x).view(batch_size, sq_len, self.n_head, self.d_key)).transpose(1, 2)\n",
    "            vs = self.v(x).view(batch_size, sq_len, self.n_head, self.d_key).transpose(1, 2)\n",
    "            return ks, vs\n",
    "        else:\n",
    "            if x.size(1) != 1:\n",
    "                raise Exception(\"Expected only one sequence element input at a time with auto-regressive kv caching\")\n",
    "            sq_len = ks.size(2)\n",
    "            new_ks = self.rope(self.k(x).view(batch_size, 1, self.n_head, self.d_key), input_pos=position).transpose(1, 2)\n",
    "            new_vs = self.v(x).view(batch_size, 1, self.n_head, self.d_key).transpose(1, 2)\n",
    "            return torch.cat([ks, new_ks], 2), torch.cat([vs, new_vs], 2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7dd37f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base model shell for embeddings, output linear, and calculating loss\n",
    "class Base(nn.Module):\n",
    "    def __init__(self, model, vocab, reuse_embeddings = False, include_x_loss = False):\n",
    "        super(Base, self).__init__()\n",
    "        self.model = model\n",
    "        self.vocab = vocab\n",
    "        self.d_model = model.d_model\n",
    "        self.vocab_len = len(vocab)\n",
    "        self.embedding = nn.Embedding(self.vocab_len, self.d_model)\n",
    "        self.actor = nn.Linear(self.d_model, self.vocab_len, bias=False)\n",
    "        if reuse_embeddings:\n",
    "            self.actor.weight = self.embedding.weight\n",
    "        self.include_x_loss = include_x_loss\n",
    "        self.criteria = nn.CrossEntropyLoss(ignore_index=vocab.index(PAD))\n",
    "\n",
    "        nn.init.normal_(self.embedding.weight, mean=0.0, std=0.02)\n",
    "        nn.init.normal_(self.actor.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, x, x_len, hiddens=None):\n",
    "        if len(x.size()) != 2:\n",
    "            raise Exception(\"Input must be of shape (batch_size, sq_len), received input shape\", x.shape()) \n",
    "        output, hiddens = self.model(self.embedding(x), x_len, hiddens)\n",
    "        return self.actor(output) / (self.d_model ** 0.5), hiddens\n",
    "    \n",
    "    def calcLoss(self, xy, x_len, xy_len):\n",
    "        batch_size = xy.size(0)\n",
    "        sq_len = xy.size(1)\n",
    "        output, _ = self(xy, xy_len)\n",
    "\n",
    "        if self.include_x_loss:\n",
    "            selected = (torch.arange(sq_len, device=xy.device).unsqueeze(0) < xy_len.unsqueeze(1))[:, 1:]\n",
    "        else:\n",
    "            selected = ((torch.arange(sq_len, device=xy.device).unsqueeze(0) < xy_len.unsqueeze(1)) & (torch.arange(sq_len, device=xy.device).unsqueeze(0) >= x_len.unsqueeze(1)))[:, 1:]\n",
    "            \n",
    "        guesses = output[:, :-1][selected]\n",
    "        actual = xy[:, 1:][selected]\n",
    "\n",
    "        return self.criteria(guesses, actual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb8b4cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder only transformer architecture, as commonly used in generative pretrained transformers \n",
    "class DecTrans(nn.Module):\n",
    "    def __init__(self, d_model, d_sa, d_ffwd, n_head, n_lay, activ=nn.GELU(), dropout = 0.1):\n",
    "        super(DecTrans, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_sa = d_sa\n",
    "        self.d_ffwd = d_ffwd\n",
    "        self.n_head = n_head\n",
    "        self.n_lay = n_lay\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.sas = nn.ParameterList([MHSA(d_model, d_sa, n_head, dropout=dropout) for _ in range(n_lay)])\n",
    "        self.ffwds = nn.ParameterList([FeedFwd([d_model, d_ffwd, d_model], dropout=dropout, activ=activ) for _ in range(n_lay)])\n",
    "        self.sa_norms = nn.ParameterList([nn.LayerNorm(d_model) for _ in range(n_lay)])\n",
    "        self.ffwd_norms = nn.ParameterList([nn.LayerNorm(d_model) for _ in range(n_lay)])\n",
    "\n",
    "        for norm in self.sa_norms:\n",
    "            nn.init.ones_(norm.weight)\n",
    "            nn.init.zeros_(norm.bias)\n",
    "\n",
    "        for norm in self.ffwd_norms:\n",
    "            nn.init.ones_(norm.weight)\n",
    "            nn.init.zeros_(norm.bias)\n",
    "        \n",
    "    def forward(self, x, x_len, hiddens=None):\n",
    "        batch_size = x.size(0)\n",
    "        sq_len = x.size(1)\n",
    "        if hiddens == None:\n",
    "            ks = [None for _ in range(self.n_lay)]\n",
    "            vs = [None for _ in range(self.n_lay)]\n",
    "            src_mask = (torch.arange(sq_len, device=x.device).unsqueeze(0) < x_len.unsqueeze(1))\n",
    "            mask = src_mask.unsqueeze(1).repeat(1, sq_len, 1) & torch.tril(torch.ones(batch_size, sq_len, sq_len, dtype=torch.bool, device=x.device))\n",
    "            position = None\n",
    "        else:\n",
    "            #TODO, allow sq_len > 1 during generation\n",
    "            if sq_len != 1:\n",
    "                raise Exception(\"Must only enter one sequence element at a time during autoregressive generation\")\n",
    "            ks, vs, src_mask = hiddens\n",
    "            src_mask = torch.cat([src_mask, (torch.arange(sq_len, device=x.device).unsqueeze(0) < x_len.unsqueeze(1))], dim=1)\n",
    "            # We don't need a causality mask when we are autoregressively generating, and when there are hiddens we must be autoregressively generating\n",
    "            mask = src_mask.unsqueeze(1)\n",
    "            position = mask.sum(dim=2) - 1\n",
    "            \n",
    "        for layer in range(self.n_lay):\n",
    "            ks[layer], vs[layer] = self.sas[layer].cache_kvs(x, ks[layer], vs[layer], position)\n",
    "            x = self.sa_norms[layer](x + self.dropout(self.sas[layer](x, ks[layer], vs[layer], mask=mask, position=position)))\n",
    "            x = self.ffwd_norms[layer](x + self.dropout(self.ffwds[layer](x)))\n",
    "        return x, (ks, vs, src_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "152b3122-8655-475e-aa85-5ce44562c8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple layer LSTM, uses residual connections and layer normalization, like the decoder transformer, but has no self attention\n",
    "class MultiLayLSTM(nn.Module):\n",
    "    def __init__(self, d_model, d_lstm, n_lay, dropout = 0.1):\n",
    "        super(MultiLayLSTM, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_lstm = d_lstm\n",
    "        self.n_lay = n_lay\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.lstms = nn.ParameterList([LSTM(d_model, d_lstm, d_model) for _ in range(n_lay)])\n",
    "        self.norms = nn.ParameterList([nn.LayerNorm(d_model) for _ in range(n_lay)])\n",
    "    \n",
    "    def forward(self, x, x_len, hiddens=None):\n",
    "        batch_size = x.size(0)\n",
    "        if hiddens == None:\n",
    "            hiddens = [None for _ in range(self.n_lay)]\n",
    "\n",
    "        for layer in range(self.n_lay):\n",
    "            lstm_out, hiddens[layer] = self.lstms[layer](x, x_len, hiddens[layer])\n",
    "            x = self.norms[layer](x + self.dropout(lstm_out))\n",
    "            \n",
    "        return x, hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d618512e-e3c8-4858-9b87-2348f3642985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long short term memory transformer, same as decoder transformer except substituting the feed forward network with an LSTM\n",
    "class LSTMTrans(nn.Module):\n",
    "    def __init__(self, d_model, d_sa, d_lstm, n_head, n_lay, activ=nn.GELU(), dropout = 0.1):\n",
    "        super(LSTMTrans, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_sa = d_sa\n",
    "        self.d_lstm = d_lstm\n",
    "        self.n_head = n_head\n",
    "        self.n_lay = n_lay\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.sas = nn.ParameterList([MHSA(d_model, d_sa, n_head, dropout=dropout) for _ in range(n_lay)])\n",
    "        self.lstms = nn.ParameterList([LSTM(d_model, d_lstm, d_model) for _ in range(n_lay)])\n",
    "        self.sa_norms = nn.ParameterList([nn.LayerNorm(d_model) for _ in range(n_lay)])\n",
    "        self.lstm_norms = nn.ParameterList([nn.LayerNorm(d_model) for _ in range(n_lay)])\n",
    "\n",
    "        for norm in self.sa_norms:\n",
    "            nn.init.ones_(norm.weight)\n",
    "            nn.init.zeros_(norm.bias)\n",
    "\n",
    "        for norm in self.lstm_norms:\n",
    "            nn.init.ones_(norm.weight)\n",
    "            nn.init.zeros_(norm.bias)\n",
    "        \n",
    "    def forward(self, x, x_len, hiddens=None):\n",
    "        batch_size = x.size(0)\n",
    "        sq_len = x.size(1)\n",
    "        if hiddens == None:\n",
    "            ks = [None for _ in range(self.n_lay)]\n",
    "            vs = [None for _ in range(self.n_lay)]\n",
    "            src_mask = (torch.arange(sq_len, device=x.device).unsqueeze(0) < x_len.unsqueeze(1))\n",
    "            mask = src_mask.unsqueeze(1).repeat(1, sq_len, 1) & torch.tril(torch.ones(batch_size, sq_len, sq_len, dtype=torch.bool, device=x.device))\n",
    "            position = None\n",
    "            lstm_hiddens = [None for _ in range(self.n_lay)]\n",
    "        else:\n",
    "            #TODO, allow sq_len > 1 during generation\n",
    "            if sq_len != 1:\n",
    "                raise Exception(\"Must only enter one sequence element at a time during autoregressive generation\")\n",
    "            ks, vs, src_mask, lstm_hiddens = hiddens\n",
    "            src_mask = torch.cat([src_mask, (torch.arange(sq_len, device=x.device).unsqueeze(0) < x_len.unsqueeze(1))], dim=1)\n",
    "            # We don't need a causality mask when we are autoregressively generating, and when there are hiddens we must be autoregressively generating\n",
    "            mask = src_mask.unsqueeze(1)\n",
    "            position = mask.sum(dim=2) - 1\n",
    "            \n",
    "        for layer in range(self.n_lay):\n",
    "            ks[layer], vs[layer] = self.sas[layer].cache_kvs(x, ks[layer], vs[layer], position)\n",
    "            x = self.sa_norms[layer](x + self.dropout(self.sas[layer](x, ks[layer], vs[layer], mask=mask, position=position)))\n",
    "            lstm_out, lstm_hiddens[layer] = self.lstms[layer](x, x_len, lstm_hiddens[layer])\n",
    "            x = self.lstm_norms[layer](x + self.dropout(lstm_out))\n",
    "        return x, (ks, vs, src_mask, lstm_hiddens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f322fed4-db37-4d12-94aa-ed1e929c00ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer with recurrent self attention, same as decoder transformer except derives keys and values from output rather than input\n",
    "class RSATrans(nn.Module):\n",
    "    def __init__(self, d_model, d_sa, d_ffwd, n_head, n_lay, activ=nn.GELU(), dropout = 0.1):\n",
    "        super(RSATrans, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ffwd = d_ffwd\n",
    "        self.d_sa = d_sa\n",
    "        self.n_head = n_head\n",
    "        self.n_lay = n_lay\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.ffwds = nn.ParameterList([FeedFwd([d_model, d_ffwd, d_model], dropout=dropout, activ=activ) for _ in range(n_lay)])\n",
    "        self.sas = nn.ParameterList([MHSA(d_model, d_sa, n_head, dropout=dropout) for _ in range (n_lay)])\n",
    "        \n",
    "        self.sa_h0s = nn.ParameterList([nn.Parameter(torch.zeros(d_model)) for _ in range(n_lay)])\n",
    "        self.sa_norms = nn.ParameterList([nn.LayerNorm(d_model) for _ in range(n_lay)])\n",
    "        self.ffwd_norms = nn.ParameterList([nn.LayerNorm(d_model) for _ in range(n_lay)])\n",
    "\n",
    "        for h0 in self.sa_h0s:\n",
    "            nn.init.normal_(h0, mean=0.0, std=0.02)\n",
    "        \n",
    "        for norm in self.sa_norms:\n",
    "            nn.init.ones_(norm.weight)\n",
    "            nn.init.zeros_(norm.bias)\n",
    "\n",
    "        for norm in self.ffwd_norms:\n",
    "            nn.init.ones_(norm.weight)\n",
    "            nn.init.zeros_(norm.bias)\n",
    "        \n",
    "    def forward(self, x, x_len, hiddens=None):\n",
    "        batch_size = x.size(0)\n",
    "        sq_len = x.size(1)\n",
    "        out = torch.zeros((batch_size, sq_len, self.d_model), device = x.device)\n",
    "        \n",
    "        if hiddens == None:\n",
    "            ks = [None for _ in range(self.n_lay)]\n",
    "            vs = [None for _ in range(self.n_lay)]\n",
    "            for layer in range(self.n_lay):\n",
    "                ks[layer], vs[layer] = self.sas[layer].cache_kvs(self.sa_h0s[layer].unsqueeze(0).repeat(batch_size, 1))\n",
    "            # When no hiddens, our source mask must be one larger in the sq_len dimension in order to accomodate the h0\n",
    "            src_mask = (torch.arange(sq_len + 1, device=x.device).unsqueeze(0) < (x_len + 1).unsqueeze(1))\n",
    "        else:\n",
    "            ks, vs, src_mask, lstm_hiddens = hiddens\n",
    "            src_mask = torch.cat([src_mask, (torch.arange(sq_len, device=x.device).unsqueeze(0) < x_len.unsqueeze(1))], dim=1)\n",
    "        \n",
    "        for sq_idx in range(sq_len):\n",
    "            position = src_mask[:, :(sq_idx - sq_len)].sum(dim=1)\n",
    "            curr = x[:, sq_idx]\n",
    "            for layer in range(self.n_lay):\n",
    "                curr = self.sa_norms[layer](curr + self.dropout(self.sas[layer](curr, ks[layer], vs[layer], mask=src_mask[:, :(sq_idx - sq_len)].unsqueeze(1), position=position)))\n",
    "                curr = self.ffwd_norms[layer](curr + self.dropout(self.ffwds[layer](curr)))\n",
    "                ks[layer], vs[layer] = self.sas[layer].cache_kvs(curr, ks[layer], vs[layer], position)\n",
    "            out[:, sq_idx] = curr\n",
    "        \n",
    "        return out, (ks, vs, src_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac589ac9-7307-4dae-a884-ae072dcbbc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent Transformer architecture, utilizes LSTMs and recurrent self attention\n",
    "class ReTrans(nn.Module):\n",
    "    def __init__(self, d_model, d_sa, d_lstm, n_head, n_lay, dropout = 0.1):\n",
    "        super(ReTrans, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_lstm = d_lstm\n",
    "        self.d_sa = d_sa\n",
    "        self.n_head = n_head\n",
    "        self.n_lay = n_lay\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.lstms = nn.ParameterList([LSTM(d_model, d_lstm, d_model) for _ in range(n_lay)])\n",
    "        self.sas = nn.ParameterList([MHSA(d_model, d_sa, n_head, dropout=dropout) for _ in range (n_lay)])\n",
    "        \n",
    "        self.sa_h0s = nn.ParameterList([nn.Parameter(torch.zeros(d_model)) for _ in range(n_lay)])\n",
    "        self.sa_norms = nn.ParameterList([nn.LayerNorm(d_model) for _ in range(n_lay)])\n",
    "        self.lstm_norms = nn.ParameterList([nn.LayerNorm(d_model) for _ in range(n_lay)])\n",
    "\n",
    "        for h0 in self.sa_h0s:\n",
    "            nn.init.normal_(h0, mean=0.0, std=0.02)\n",
    "        \n",
    "        for norm in self.sa_norms:\n",
    "            nn.init.ones_(norm.weight)\n",
    "            nn.init.zeros_(norm.bias)\n",
    "\n",
    "        for norm in self.lstm_norms:\n",
    "            nn.init.ones_(norm.weight)\n",
    "            nn.init.zeros_(norm.bias)\n",
    "        \n",
    "    def forward(self, x, x_len, hiddens=None):\n",
    "        batch_size = x.size(0)\n",
    "        sq_len = x.size(1)\n",
    "        out = torch.zeros((batch_size, sq_len, self.d_model), device = x.device)\n",
    "        \n",
    "        if hiddens == None:\n",
    "            ks = [None for _ in range(self.n_lay)]\n",
    "            vs = [None for _ in range(self.n_lay)]\n",
    "            lstm_hiddens = [None for _ in range(self.n_lay)]\n",
    "            for layer in range(self.n_lay):\n",
    "                ks[layer], vs[layer] = self.sas[layer].cache_kvs(self.sa_h0s[layer].unsqueeze(0).repeat(batch_size, 1))\n",
    "            # When no hiddens, our source mask must be one larger in the sq_len dimension in order to accomodate the h0\n",
    "            src_mask = (torch.arange(sq_len + 1, device=x.device).unsqueeze(0) < (x_len + 1).unsqueeze(1))\n",
    "        else:\n",
    "            ks, vs, src_mask, lstm_hiddens = hiddens\n",
    "            src_mask = torch.cat([src_mask, (torch.arange(sq_len, device=x.device).unsqueeze(0) < x_len.unsqueeze(1))], dim=1)\n",
    "        \n",
    "        for sq_idx in range(sq_len):\n",
    "            position = src_mask[:, :(sq_idx - sq_len)].sum(dim=1)\n",
    "            curr = x[:, sq_idx]\n",
    "            for layer in range(self.n_lay):\n",
    "                curr = self.sa_norms[layer](curr + self.dropout(self.sas[layer](curr, ks[layer], vs[layer], mask=src_mask[:, :(sq_idx - sq_len)].unsqueeze(1), position=position)))\n",
    "                lstm_out, lstm_hiddens[layer] = self.lstms[layer](curr.unsqueeze(1), src_mask[:, (sq_idx - sq_len)], lstm_hiddens[layer])\n",
    "                curr = self.lstm_norms[layer](curr + self.dropout(lstm_out.squeeze(1)))\n",
    "                ks[layer], vs[layer] = self.sas[layer].cache_kvs(curr, ks[layer], vs[layer], position)\n",
    "            out[:, sq_idx] = curr\n",
    "        \n",
    "        return out, (ks, vs, src_mask, lstm_hiddens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983bc756-8161-4316-82d6-e3bc3a998145",
   "metadata": {},
   "source": [
    "# Training + Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f01f1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different schedulers, we only end up using linear,\n",
    "# But decay and cosine are included in case \n",
    "def decay_scheduler(steps):\n",
    "    warmup_steps = steps // 10\n",
    "    decay = 0.05 ** (1 / (steps - warmup_steps))\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < warmup_steps:\n",
    "            return current_step / warmup_steps\n",
    "        return decay ** (current_step - warmup_steps)\n",
    "    return lr_lambda\n",
    "\n",
    "def cosine_scheduler(steps):\n",
    "    warmup_steps = steps // 10\n",
    "    min_lr = 0.05\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < warmup_steps:\n",
    "            return current_step / warmup_steps\n",
    "        return min_lr + (1 - min_lr) / 2 * (1 + math.cos((current_step - warmup_steps) * math.pi / (steps - warmup_steps)))\n",
    "    return lr_lambda\n",
    "\n",
    "def linear_scheduler(steps):\n",
    "    warmup_steps = steps // 10\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < warmup_steps:\n",
    "            return current_step / warmup_steps\n",
    "        return 1.0 - ((current_step - warmup_steps) / (steps - warmup_steps))\n",
    "    return lr_lambda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30c30285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train function, learning rate scheduling is done on a batch basis instead of epoch, revert to best model\n",
    "def train(model, trainset, validset, lr=0.001, batch_size=512, epochs=10, optimizer=torch.optim.AdamW, scheduler=cosine_scheduler):\n",
    "    train_iter = DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    valid_iter = DataLoader(validset, batch_size=batch_size, pin_memory=True)\n",
    "    optim = optimizer(model.parameters(), lr=lr)\n",
    "    lambdalr = torch.optim.lr_scheduler.LambdaLR(optim, lr_lambda=scheduler(epochs * len(train_iter)))\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_idx, batch in enumerate(train_iter):\n",
    "            xy, x_len, xy_len = batch\n",
    "            print('=' * (math.floor((batch_idx / len(train_iter)) * 40) - math.floor(((batch_idx - 1) / len(train_iter)) * 40)), end = \"\")\n",
    "            optim.zero_grad()\n",
    "            loss = model.calcLoss(xy.to(device), x_len.to(device), xy_len.to(device))\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            lambdalr.step()\n",
    "        print(\">\\nEpoch {} train loss:\\t{}\".format(epoch, train_loss / len(train_iter)))\n",
    "        train_losses.append(train_loss / len(train_iter))\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_loss = 0\n",
    "            for batch in valid_iter:\n",
    "                xy, x_len, xy_len = batch\n",
    "                loss = model.calcLoss(xy.to(device), x_len.to(device), xy_len.to(device))\n",
    "                valid_loss += loss.item()\n",
    "        print(\"Epoch {} valid loss:\\t{}\".format(epoch, valid_loss / len(valid_iter)))\n",
    "        valid_losses.append(valid_loss / len(valid_iter))\n",
    "        \n",
    "        if valid_losses[-1] == min(valid_losses):\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "    model.load_state_dict(torch.load('best_model.pth', weights_only=True))\n",
    "    os.remove('best_model.pth')\n",
    "    return train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3f95ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests what proportion of the test set that the model predicts in one shot\n",
    "@torch.no_grad()\n",
    "def test(model, testset, batch_size=512):\n",
    "    test_iter = DataLoader(testset, batch_size=batch_size, pin_memory=True)\n",
    "    num_correct = 0\n",
    "    model.eval()\n",
    "    for batch in test_iter:\n",
    "        xy, x_len, xy_len = batch\n",
    "        xy = xy.to(device)\n",
    "        x_len = x_len.to(device)\n",
    "        xy_len = xy_len.to(device)\n",
    "        out, _ = model(xy, xy_len)\n",
    "        guesses = torch.argmax(out[:, :-1], dim=-1)\n",
    "        answer_selector = ((torch.arange(xy.size(1), device=device).unsqueeze(0) < xy_len.unsqueeze(1)) & (torch.arange(xy.size(1), device=device).unsqueeze(0) >= x_len.unsqueeze(1))).to(device)[:, 1:]\n",
    "        correct_token = (guesses == xy[:, 1:]) & answer_selector\n",
    "        correct_answer = (correct_token.sum(dim=1) == (xy_len - x_len))\n",
    "        num_correct += torch.sum(correct_answer).item()\n",
    "    return num_correct / len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f4aa0c8-2af1-410f-8afe-05ded4bddb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the output for an input\n",
    "@torch.no_grad()\n",
    "def solve(model, x, x_len, vocab, max_y_len):\n",
    "    batch_size = x.size(0)\n",
    "    stop_idx = vocab.index(STOP)\n",
    "    y = torch.zeros(batch_size, max_y_len, dtype=torch.long, device=x.device)\n",
    "    y_len = torch.full((batch_size,), max_y_len, device=x.device)\n",
    "    model.eval()\n",
    "    out, hiddens = model(x, x_len)\n",
    "    y[:, 0] = torch.argmax(out[torch.arange(batch_size, device=x.device), x_len - 1], dim=1)\n",
    "    for i in range(max_y_len - 1):\n",
    "        y_len[(y[:, i] == stop_idx) & (y_len == max_y_len)] = i\n",
    "        out, hiddens = model(y[:, i].unsqueeze(1), (y_len == max_y_len), hiddens)\n",
    "        y[:, i + 1] = torch.argmax(out[:, 0], dim=1)\n",
    "    return y, y_len + 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e50add2",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f92fbc05-5d97-4a22-91b0-1cabec2c7a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(datasets, model_type, model_args, train_args={}, test_args={}):\n",
    "    trainset, validset, testset = datasets\n",
    "    experimental_results = {}\n",
    "    if str(device) == \"cpu\":\n",
    "        experimental_results[\"device\"] = \"cpu\"\n",
    "    else:\n",
    "        experimental_results[\"device\"] = torch.cuda.get_device_name(torch.cuda.current_device())\n",
    "    experimental_results[\"model class\"] = model_type.__name__\n",
    "    experimental_results[\"dataset class\"] = trainset.dataset.__class__.__name__\n",
    "    experimental_results[\"dataset sizes\"] = (len(trainset), len(validset), len(testset))\n",
    "    model = Base(model_type(**model_args), trainset.dataset.vocab).to(device)\n",
    "    experimental_results[\"model args\"] = model_args\n",
    "    experimental_results[\"model size\"] = sum(p.numel() for p in model.model.parameters() if p.requires_grad)\n",
    "    print(\"Parameter count\", experimental_results[\"model size\"])\n",
    "    flop_xy, flop_x_len, flop_xy_len = next(iter(DataLoader(testset, batch_size=64)))\n",
    "    # Note this does not catch all operations, but it catches matmul, which is by far the dominant operator\n",
    "    flops = FlopCountAnalysis(model, (flop_xy.to(device), flop_xy_len.to(device)))\n",
    "    experimental_results[\"flops per input\"] = flops.total() // 64\n",
    "    print(\"Flops per input\", experimental_results[\"flops per input\"])\n",
    "    experimental_results[\"train args\"] = train_args\n",
    "    start_time = time.perf_counter()\n",
    "    train_loss, valid_loss = train(model, trainset, validset, **train_args)\n",
    "    end_time = time.perf_counter()\n",
    "    experimental_results[\"train loss\"] = train_loss\n",
    "    experimental_results[\"valid loss\"] = valid_loss\n",
    "    experimental_results[\"train time\"] = end_time - start_time\n",
    "    print(\"Train time\", experimental_results[\"train time\"])\n",
    "    experimental_results[\"test args\"] = test_args\n",
    "    experimental_results[\"test accuracy\"] = test(model, testset, **test_args)\n",
    "    print(\"Test accuracy\", experimental_results[\"test accuracy\"])\n",
    "    return experimental_results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20eec259-e797-4f92-8b9a-fe435d8442c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_experiment(experiment_dict, name=None):\n",
    "    if not os.path.exists(\"experiments\"):\n",
    "        os.makedirs(\"experiments\")\n",
    "    if name == None:\n",
    "        name = time.ctime()\n",
    "    try:\n",
    "        with open(f\"experiments/{time.perf_counter() if name == None else name}.json\", \"w\") as f:\n",
    "            json.dump(experiment_dict, f, indent=4, default=lambda x: x.__name__)\n",
    "        print(f\"Experiment {name} saved\")\n",
    "    except:\n",
    "        print(f\"Failure saving experiment {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ebf88ec-c812-4b81-acd7-69127428663f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_experiments():\n",
    "    experiments = []\n",
    "    if not os.path.exists(\"experiments\"):\n",
    "        print(\"No experiments directory\")\n",
    "    else:\n",
    "        for file in os.listdir(\"experiments\"):\n",
    "            try:\n",
    "                with open(f\"experiments/{file}\", \"r\") as f:\n",
    "                    experiments.append(json.load(f))\n",
    "            except:\n",
    "                print(\"Error loading json experiment:\", file)\n",
    "    return experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c29f53c-dd80-493b-81dc-181c72a99a51",
   "metadata": {},
   "source": [
    "## Arith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b32d90c-0d34-4960-80ac-65c6f1a0c6da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c14177f1-49b6-452a-99c2-df0d31aad559",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::embedding encountered 1 time(s)\n",
      "Unsupported operator aten::lt encountered 1 time(s)\n",
      "Unsupported operator aten::repeat encountered 4 time(s)\n",
      "Unsupported operator aten::tril encountered 1 time(s)\n",
      "Unsupported operator aten::mul encountered 24 time(s)\n",
      "Unsupported operator aten::sub encountered 6 time(s)\n",
      "Unsupported operator aten::add encountered 12 time(s)\n",
      "Unsupported operator aten::div encountered 4 time(s)\n",
      "Unsupported operator aten::softmax encountered 3 time(s)\n",
      "Unsupported operator aten::gelu encountered 3 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "criteria\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter count 149952\n",
      "Flops per input 6281856\n",
      "========================================>\n",
      "Epoch 0 train loss:\t1.8417049192190171\n",
      "Epoch 0 valid loss:\t1.3440140140056611\n",
      "========================================>\n",
      "Epoch 1 train loss:\t1.2594315618276597\n",
      "Epoch 1 valid loss:\t1.0930068480968476\n",
      "========================================>\n",
      "Epoch 2 train loss:\t1.1251337978839875\n",
      "Epoch 2 valid loss:\t0.9957554686069489\n",
      "========================================>\n",
      "Epoch 3 train loss:\t1.0633177067041397\n",
      "Epoch 3 valid loss:\t0.9431927040219307\n",
      "========================================>\n",
      "Epoch 4 train loss:\t1.0187888298630714\n",
      "Epoch 4 valid loss:\t0.9007713472843171\n",
      "========================================>\n",
      "Epoch 5 train loss:\t0.9822534518241882\n",
      "Epoch 5 valid loss:\t0.8559731432795524\n",
      "========================================>\n",
      "Epoch 6 train loss:\t0.9513747674226761\n",
      "Epoch 6 valid loss:\t0.8288433396816254\n",
      "========================================>\n",
      "Epoch 7 train loss:\t0.9263836594223976\n",
      "Epoch 7 valid loss:\t0.8064783629775047\n",
      "========================================>\n",
      "Epoch 8 train loss:\t0.9094846810698509\n",
      "Epoch 8 valid loss:\t0.7904303082823754\n",
      "========================================>\n",
      "Epoch 9 train loss:\t0.9002409046888351\n",
      "Epoch 9 valid loss:\t0.7869286978244782\n",
      "Train time 558.022420887\n",
      "Test accuracy 0.38666015625\n",
      "Experiment Fri Jun 27 09:22:23 2025 saved\n"
     ]
    }
   ],
   "source": [
    "write_experiment(experiment(arithSets, DecTrans, \\\n",
    "                            model_args={\"d_model\":64, \"d_sa\":64, \"d_ffwd\":256, \"n_head\":8, \"n_lay\":3}, \\\n",
    "                            train_args={\"lr\":0.0045, \"batch_size\":512, \"epochs\":10}, \\\n",
    "                            test_args={\"batch_size\":512}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f9fbf51d-21b0-4365-b0f3-1aec84ffa985",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::embedding encountered 1 time(s)\n",
      "Unsupported operator aten::lt encountered 1 time(s)\n",
      "Unsupported operator aten::repeat encountered 6 time(s)\n",
      "Unsupported operator aten::tril encountered 1 time(s)\n",
      "Unsupported operator aten::mul encountered 40 time(s)\n",
      "Unsupported operator aten::sub encountered 10 time(s)\n",
      "Unsupported operator aten::add encountered 20 time(s)\n",
      "Unsupported operator aten::div encountered 6 time(s)\n",
      "Unsupported operator aten::softmax encountered 5 time(s)\n",
      "Unsupported operator aten::gelu encountered 5 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "criteria\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter count 3948800\n",
      "Flops per input 153809408\n",
      "========================================>\n",
      "Epoch 0 train loss:\t1.9374132400751114\n",
      "Epoch 0 valid loss:\t1.246037756204605\n",
      "========================================>\n",
      "Epoch 1 train loss:\t1.0938537355661393\n",
      "Epoch 1 valid loss:\t0.9020304915308952\n",
      "========================================>\n",
      "Epoch 2 train loss:\t0.8818446969985962\n",
      "Epoch 2 valid loss:\t0.7396587410569191\n",
      "========================================>\n",
      "Epoch 3 train loss:\t0.7732023388743401\n",
      "Epoch 3 valid loss:\t0.662105156481266\n",
      "========================================>\n",
      "Epoch 4 train loss:\t0.6951666669845581\n",
      "Epoch 4 valid loss:\t0.5851720321178436\n",
      "========================================>\n",
      "Epoch 5 train loss:\t0.6263451393246651\n",
      "Epoch 5 valid loss:\t0.5259483921527862\n",
      "========================================>\n",
      "Epoch 6 train loss:\t0.5714595712125301\n",
      "Epoch 6 valid loss:\t0.47787079453468323\n",
      "========================================>\n",
      "Epoch 7 train loss:\t0.5273010793030262\n",
      "Epoch 7 valid loss:\t0.446413602232933\n",
      "========================================>\n",
      "Epoch 8 train loss:\t0.4973969295620918\n",
      "Epoch 8 valid loss:\t0.42570102229714396\n",
      "========================================>\n",
      "Epoch 9 train loss:\t0.4810169892013073\n",
      "Epoch 9 valid loss:\t0.4167187611758709\n",
      "Train time 3422.1895275879997\n",
      "Test accuracy 0.64896484375\n",
      "Experiment Fri Jun 27 15:58:20 2025 saved\n"
     ]
    }
   ],
   "source": [
    "write_experiment(experiment(arithSets, DecTrans, \\\n",
    "                            model_args={\"d_model\":256, \"d_sa\":256, \"d_ffwd\":1024, \"n_head\":8, \"n_lay\":5}, \\\n",
    "                            train_args={\"lr\":0.001, \"batch_size\":512, \"epochs\":10}, \\\n",
    "                            test_args={\"batch_size\":512}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "215dab4c-baaf-4925-9b3f-80000bc14826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter count 125280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::embedding encountered 1 time(s)\n",
      "Unsupported operator aten::repeat encountered 6 time(s)\n",
      "Unsupported operator aten::lt encountered 3 time(s)\n",
      "Unsupported operator aten::add encountered 801 time(s)\n",
      "Unsupported operator aten::sigmoid encountered 342 time(s)\n",
      "Unsupported operator aten::tanh encountered 114 time(s)\n",
      "Unsupported operator aten::mul encountered 912 time(s)\n",
      "Unsupported operator aten::div encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "criteria\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flops per input 4713216\n",
      "========================================>\n",
      "Epoch 0 train loss:\t1.701649946808815\n",
      "Epoch 0 valid loss:\t1.0612090414762496\n",
      "========================================>\n",
      "Epoch 1 train loss:\t0.9610943623185157\n",
      "Epoch 1 valid loss:\t0.8561809068918228\n",
      "========================================>\n",
      "Epoch 2 train loss:\t0.8372959240674972\n",
      "Epoch 2 valid loss:\t0.7597445115447045\n",
      "========================================>\n",
      "Epoch 3 train loss:\t0.7489423130750656\n",
      "Epoch 3 valid loss:\t0.7262473955750466\n",
      "========================================>\n",
      "Epoch 4 train loss:\t0.6771923592686653\n",
      "Epoch 4 valid loss:\t0.6312348434329033\n",
      "========================================>\n",
      "Epoch 5 train loss:\t0.6131974802017212\n",
      "Epoch 5 valid loss:\t0.5716911020874977\n",
      "========================================>\n",
      "Epoch 6 train loss:\t0.5747502877414227\n",
      "Epoch 6 valid loss:\t0.5362723788619042\n",
      "========================================>\n",
      "Epoch 7 train loss:\t0.5371888148486614\n",
      "Epoch 7 valid loss:\t0.5043300920724869\n",
      "========================================>\n",
      "Epoch 8 train loss:\t0.5154420385658741\n",
      "Epoch 8 valid loss:\t0.49001828908920286\n",
      "========================================>\n",
      "Epoch 9 train loss:\t0.500849281013012\n",
      "Epoch 9 valid loss:\t0.48158605173230173\n",
      "Train time 2112.798825157999\n",
      "Test accuracy 0.59642578125\n",
      "Experiment Thu Jun 26 15:51:28 2025 saved\n"
     ]
    }
   ],
   "source": [
    "write_experiment(experiment(arithSets, MultiLayLSTM, \\\n",
    "                            model_args={\"d_model\":32, \"d_lstm\":128, \"n_lay\":3}, \\\n",
    "                            train_args={\"lr\":0.005, \"batch_size\":512, \"epochs\":10}, \\\n",
    "                            test_args={\"batch_size\":512}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "677ef907-553c-4192-b0e0-a69a022a421e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter count 3292800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::embedding encountered 1 time(s)\n",
      "Unsupported operator aten::repeat encountered 10 time(s)\n",
      "Unsupported operator aten::lt encountered 5 time(s)\n",
      "Unsupported operator aten::add encountered 1335 time(s)\n",
      "Unsupported operator aten::sigmoid encountered 570 time(s)\n",
      "Unsupported operator aten::tanh encountered 190 time(s)\n",
      "Unsupported operator aten::mul encountered 1520 time(s)\n",
      "Unsupported operator aten::div encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "criteria\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flops per input 124742144\n",
      "========================================>\n",
      "Epoch 0 train loss:\t1.868000888943672\n",
      "Epoch 0 valid loss:\t0.9733535534143448\n",
      "========================================>\n",
      "Epoch 1 train loss:\t0.7860862409472466\n",
      "Epoch 1 valid loss:\t0.6519121026992798\n",
      "========================================>\n",
      "Epoch 2 train loss:\t0.5713275528252125\n",
      "Epoch 2 valid loss:\t0.48847379624843595\n",
      "========================================>\n",
      "Epoch 3 train loss:\t0.4576201938986778\n",
      "Epoch 3 valid loss:\t0.41708282977342603\n",
      "========================================>\n",
      "Epoch 4 train loss:\t0.39846233794093133\n",
      "Epoch 4 valid loss:\t0.3835963936150074\n",
      "========================================>\n",
      "Epoch 5 train loss:\t0.35222368547320365\n",
      "Epoch 5 valid loss:\t0.3436496131122112\n",
      "========================================>\n",
      "Epoch 6 train loss:\t0.31572686007618905\n",
      "Epoch 6 valid loss:\t0.32277356922626493\n",
      "========================================>\n",
      "Epoch 7 train loss:\t0.28299701888859274\n",
      "Epoch 7 valid loss:\t0.30748875573277473\n",
      "========================================>\n",
      "Epoch 8 train loss:\t0.25784314280748366\n",
      "Epoch 8 valid loss:\t0.2995294167846441\n",
      "========================================>\n",
      "Epoch 9 train loss:\t0.24241191078722477\n",
      "Epoch 9 valid loss:\t0.2990663042664528\n",
      "Train time 7937.792801912001\n",
      "Test accuracy 0.7368359375\n",
      "Experiment Sat Jun 28 10:36:05 2025 saved\n"
     ]
    }
   ],
   "source": [
    "write_experiment(experiment(arithSets, MultiLayLSTM, \\\n",
    "                            model_args={\"d_model\":128, \"d_lstm\":512, \"n_lay\":5}, \\\n",
    "                            train_args={\"lr\":0.0012, \"batch_size\":512, \"epochs\":10}, \\\n",
    "                            test_args={\"batch_size\":512}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7c9241b8-ee73-47dd-9992-b34a8fd7a5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter count 138240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::embedding encountered 1 time(s)\n",
      "Unsupported operator aten::repeat encountered 123 time(s)\n",
      "Unsupported operator aten::mul encountered 1836 time(s)\n",
      "Unsupported operator aten::sub encountered 231 time(s)\n",
      "Unsupported operator aten::add encountered 1259 time(s)\n",
      "Unsupported operator aten::lt encountered 115 time(s)\n",
      "Unsupported operator aten::rsub encountered 266 time(s)\n",
      "Unsupported operator aten::sum encountered 38 time(s)\n",
      "Unsupported operator aten::div encountered 115 time(s)\n",
      "Unsupported operator aten::softmax encountered 114 time(s)\n",
      "Unsupported operator aten::sigmoid encountered 342 time(s)\n",
      "Unsupported operator aten::tanh encountered 114 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "criteria\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flops per input 5346816\n",
      "========================================>\n",
      "Epoch 0 train loss:\t1.592698360145092\n",
      "Epoch 0 valid loss:\t0.994374129474163\n",
      "========================================>\n",
      "Epoch 1 train loss:\t0.8846483284831047\n",
      "Epoch 1 valid loss:\t0.7818192127346992\n",
      "========================================>\n",
      "Epoch 2 train loss:\t0.7689455340504646\n",
      "Epoch 2 valid loss:\t0.6894319358468056\n",
      "========================================>\n",
      "Epoch 3 train loss:\t0.6852209228277206\n",
      "Epoch 3 valid loss:\t0.6131509408354759\n",
      "========================================>\n",
      "Epoch 4 train loss:\t0.62722352206707\n",
      "Epoch 4 valid loss:\t0.5685364305973053\n",
      "========================================>\n",
      "Epoch 5 train loss:\t0.5756142050623894\n",
      "Epoch 5 valid loss:\t0.5409920769929886\n",
      "========================================>\n",
      "Epoch 6 train loss:\t0.5316275118291378\n",
      "Epoch 6 valid loss:\t0.48414146482944487\n",
      "========================================>\n",
      "Epoch 7 train loss:\t0.4964698721766472\n",
      "Epoch 7 valid loss:\t0.45638257563114165\n",
      "========================================>\n",
      "Epoch 8 train loss:\t0.47090624588727953\n",
      "Epoch 8 valid loss:\t0.4395625071227551\n",
      "========================================>\n",
      "Epoch 9 train loss:\t0.45478745993971825\n",
      "Epoch 9 valid loss:\t0.42982307136058806\n",
      "Train time 5390.705389450995\n",
      "Test accuracy 0.62669921875\n",
      "Experiment Fri Jun 27 01:43:02 2025 saved\n"
     ]
    }
   ],
   "source": [
    "write_experiment(experiment(arithSets, ReTrans, \\\n",
    "                            model_args={\"d_model\":32, \"d_sa\":32, \"d_lstm\":128, \"n_head\":8, \"n_lay\":3}, \\\n",
    "                            train_args={\"lr\":0.007, \"batch_size\":512, \"epochs\":10}, \\\n",
    "                            test_args={\"batch_size\":2048}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e8dd7275-768a-4c7b-ad72-30fc89082d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter count 3624960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::embedding encountered 1 time(s)\n",
      "Unsupported operator aten::repeat encountered 205 time(s)\n",
      "Unsupported operator aten::mul encountered 3060 time(s)\n",
      "Unsupported operator aten::sub encountered 385 time(s)\n",
      "Unsupported operator aten::add encountered 2097 time(s)\n",
      "Unsupported operator aten::lt encountered 191 time(s)\n",
      "Unsupported operator aten::rsub encountered 418 time(s)\n",
      "Unsupported operator aten::sum encountered 38 time(s)\n",
      "Unsupported operator aten::div encountered 191 time(s)\n",
      "Unsupported operator aten::softmax encountered 190 time(s)\n",
      "Unsupported operator aten::sigmoid encountered 570 time(s)\n",
      "Unsupported operator aten::tanh encountered 190 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "criteria\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flops per input 138427904\n",
      "========================================>\n",
      "Epoch 0 train loss:\t1.9051358917951584\n",
      "Epoch 0 valid loss:\t0.9930895912647247\n",
      "========================================>\n",
      "Epoch 1 train loss:\t0.772993928015232\n",
      "Epoch 1 valid loss:\t0.6251624876260757\n",
      "========================================>\n",
      "Epoch 2 train loss:\t0.5327748369574546\n",
      "Epoch 2 valid loss:\t0.4685129138827324\n",
      "========================================>\n",
      "Epoch 3 train loss:\t0.4341516872644424\n",
      "Epoch 3 valid loss:\t0.40462485402822496\n",
      "========================================>\n",
      "Epoch 4 train loss:\t0.3809758635759354\n",
      "Epoch 4 valid loss:\t0.36293858766555787\n",
      "========================================>\n",
      "Epoch 5 train loss:\t0.33573005864024164\n",
      "Epoch 5 valid loss:\t0.3353347238898277\n",
      "========================================>\n",
      "Epoch 6 train loss:\t0.30023214226961137\n",
      "Epoch 6 valid loss:\t0.3095379365980625\n",
      "========================================>\n",
      "Epoch 7 train loss:\t0.27054723016917703\n",
      "Epoch 7 valid loss:\t0.2928504400700331\n",
      "========================================>\n",
      "Epoch 8 train loss:\t0.2457404037564993\n",
      "Epoch 8 valid loss:\t0.28811739198863506\n",
      "========================================>\n",
      "Epoch 9 train loss:\t0.23002730762958526\n",
      "Epoch 9 valid loss:\t0.28693787306547164\n",
      "Train time 11419.013442933006\n",
      "Test accuracy 0.749453125\n",
      "Experiment Sat Jun 28 17:19:42 2025 saved\n"
     ]
    }
   ],
   "source": [
    "write_experiment(experiment(arithSets, ReTrans, \\\n",
    "                            model_args={\"d_model\":128, \"d_sa\":128, \"d_lstm\":512, \"n_head\":8, \"n_lay\":5}, \\\n",
    "                            train_args={\"lr\":0.0012, \"batch_size\":512, \"epochs\":10}, \\\n",
    "                            test_args={\"batch_size\":512}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c97e29-eea9-4e40-80ec-5cd34b336798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter count 3624960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::embedding encountered 1 time(s)\n",
      "Unsupported operator aten::repeat encountered 205 time(s)\n",
      "Unsupported operator aten::mul encountered 3060 time(s)\n",
      "Unsupported operator aten::sub encountered 385 time(s)\n",
      "Unsupported operator aten::add encountered 2097 time(s)\n",
      "Unsupported operator aten::lt encountered 191 time(s)\n",
      "Unsupported operator aten::rsub encountered 418 time(s)\n",
      "Unsupported operator aten::sum encountered 38 time(s)\n",
      "Unsupported operator aten::div encountered 191 time(s)\n",
      "Unsupported operator aten::softmax encountered 190 time(s)\n",
      "Unsupported operator aten::sigmoid encountered 570 time(s)\n",
      "Unsupported operator aten::tanh encountered 190 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "criteria\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flops per input 138427904\n",
      "=="
     ]
    }
   ],
   "source": [
    "write_experiment(experiment(arithSets, ReTrans, \\\n",
    "                            model_args={\"d_model\":128, \"d_sa\":128, \"d_lstm\":512, \"n_head\":8, \"n_lay\":5}, \\\n",
    "                            train_args={\"lr\":0.0011, \"batch_size\":512, \"epochs\":10}, \\\n",
    "                            test_args={\"batch_size\":512}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c812983-27a0-403a-b6a5-8f7119175b48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
